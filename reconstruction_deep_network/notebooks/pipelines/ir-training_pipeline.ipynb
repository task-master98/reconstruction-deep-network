{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "738ce4fc-fe5f-4727-8c8b-ae40e983a9e1",
   "metadata": {},
   "source": [
    "## **Training Pipeline**\n",
    "This notebook will be used to train the diffusion model using the defined train script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bb005-3c4a-4ad5-a221-28ada9351afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5275b-95a9-44b0-90f4-b8d46118f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jupyter-group3/reconstruction/reconstruction-deep-network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7325213-6f45-430e-98e0-5d6e96ea1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from argparse import ArgumentParser\n",
    "import yaml\n",
    "\n",
    "import reconstruction_deep_network\n",
    "from reconstruction_deep_network.data_loader.custom_loader import CustomDataLoader\n",
    "from reconstruction_deep_network.trainer.trainer import ModelTrainer\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802708b-1bb6-4280-b29b-043cfd862521",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark =  True\n",
    "torch.backends.cudnn.enabled =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996bae9-eeca-4079-80da-5ec7be07f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = reconstruction_deep_network.__path__[0]\n",
    "root_dir = os.path.dirname(module_path)\n",
    "data_path = os.path.join(root_dir, \"data\", \"v1\")\n",
    "text_embeddings = os.path.join(data_path, \"text_embeddings\")\n",
    "null_embeddings = os.path.join(text_embeddings, \"null\")\n",
    "if not os.path.isdir(null_embeddings):\n",
    "    os.makedirs(null_embeddings)\n",
    "trainer_config_path = os.path.join(module_path, \"trainer\", \"trainer_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e073c7-a904-4762-86a8-c9d8f472dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args=None):\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    parser.add_argument(\"--main_config_path\", type = str, dest = \"main_config_path\")\n",
    "    parser.add_argument(\"--train_metadata\", type = str, dest = \"train_metadata\")\n",
    "    parser.add_argument(\"--val_metadata\", type = str, dest = \"val_metadata\")\n",
    "    parser.add_argument(\"--num_workers\", type = int, dest = \"num_workers\")\n",
    "    parser.add_argument(\"--exp_name\", dest = \"exp_name\", type = str)\n",
    "    parser.add_argument(\"--batch_size\", dest = \"batch_size\", type = int)\n",
    "    parser.add_argument(\"--n_epochs\", dest = \"n_epochs\", type = int)\n",
    "    parser.add_argument(\"--learning_rate\", dest = \"learning_rate\", type = float)\n",
    "    parser.add_argument(\"--ckpt_path\", dest = \"ckpt_path\", type = str)\n",
    "\n",
    "    args = pl.Trainer.parse_argparser(parser.parse_args())\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef3d4a8-b50e-499c-a295-92ea9dacd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    config_file_path = args.main_config_path\n",
    "    with open(config_file_path, 'r') as f:\n",
    "        config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "    \n",
    "    config[\"train\"][\"learning_rate\"] = args.learning_rate\n",
    "    config[\"train\"][\"max_epochs\"] = args.n_epochs\n",
    "    config[\"train\"][\"batch_size\"] = args.batch_size\n",
    "\n",
    "    train_dataset = CustomDataLoader(mode = \"train\", debug = False, metadata_filename = args.train_metadata, num_views = args.num_views)\n",
    "    train_indices = list(range(0, 100))\n",
    "    train_dataset = Subset(train_dataset, train_indices)\n",
    "    print(f\"Size of train dataset: {len(train_dataset)}\")\n",
    "#     val_dataset = CustomDataLoader(mode = \"val\", debug = False, metadata_filename = args.val_metadata, num_views = args.num_views)    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size = config[\"train\"][\"batch_size\"],\n",
    "                    shuffle = True,\n",
    "                    num_workers = args.num_workers,\n",
    "                    drop_last = True)\n",
    "    \n",
    "#     val_loader = torch.utils.data.DataLoader(\n",
    "#                     val_dataset,\n",
    "#                     batch_size = 1,\n",
    "#                     shuffle = False,\n",
    "#                     num_workers = args.num_workers,\n",
    "#                     drop_last = False)\n",
    "    \n",
    "#     torch.cuda.empty_cache()\n",
    "    model_trainer = ModelTrainer()\n",
    "\n",
    "        \n",
    "    print(f\"Training for {model_trainer.max_epochs} epochs...\")\n",
    "    print(f\"Diffusion Training timesteps: {model_trainer.scheduler.num_train_timesteps}\")\n",
    "    \n",
    "    \n",
    "    ckpt_path = None if args.ckpt_path == \"None\" else args.ckpt_path\n",
    "    if ckpt_path is not None:\n",
    "        model_trainer.load_state_dict(torch.load(args.ckpt_path, map_location='cpu')[\n",
    "            'state_dict'], strict=False)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"train_loss\",\n",
    "                                          mode=\"min\", save_last=1,\n",
    "                                          filename='epoch={epoch}-loss={train_loss:.4f}')\n",
    "    \n",
    "\n",
    "#     logger = TensorBoardLogger(\n",
    "#         save_dir='logs/tb_logs', name=args.exp_name, default_hp_metric=False)\n",
    "    \n",
    "    training_pipeline = pl.Trainer.from_argparse_args(\n",
    "        args,\n",
    "        callbacks=[checkpoint_callback],\n",
    "#         limit_train_batches=1,\n",
    "#         strategy = \"ddp_notebook\",\n",
    "        amp_backend=\"apex\",\n",
    "        amp_level=\"O2\"\n",
    "        )\n",
    "    \n",
    "    training_pipeline.fit(model_trainer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deac8c1-49c1-4ad4-bd3b-fa414406207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser.add_argument(\"--main_config_path\", type = str, dest = \"main_config_path\")\n",
    "parser.add_argument(\"--train_metadata\", type = str, dest = \"train_metadata\")\n",
    "parser.add_argument(\"--val_metadata\", type = str, dest = \"val_metadata\")\n",
    "parser.add_argument(\"--num_views\", type = int, dest = \"num_views\")\n",
    "parser.add_argument(\"--num_workers\", type = int, dest = \"num_workers\")\n",
    "parser.add_argument(\"--exp_name\", dest = \"exp_name\", type = str)\n",
    "parser.add_argument(\"--batch_size\", dest = \"batch_size\", type = int)\n",
    "parser.add_argument(\"--n_epochs\", dest = \"n_epochs\", type = int)\n",
    "parser.add_argument(\"--learning_rate\", dest = \"learning_rate\", type = float)\n",
    "parser.add_argument(\"--ckpt_path\", dest = \"ckpt_path\", type = str)\n",
    "\n",
    "args = pl.Trainer.parse_argparser(parser.parse_args([\n",
    "    \"--main_config_path\", trainer_config_path,\n",
    "    \"--train_metadata\", \"ir-20231129-train-split\",\n",
    "    \"--val_metadata\", \"ir-20231129-val-split\",\n",
    "    \"--num_views\", \"1\",\n",
    "    \"--num_workers\", \"12\",\n",
    "    \"--exp_name\", \"ir-training-pipeline-test\",\n",
    "    \"--batch_size\", \"1\",\n",
    "    \"--n_epochs\", \"10\",\n",
    "    \"--learning_rate\", \"0.0002\",\n",
    "    \"--ckpt_path\", \"None\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a894ce-bf12-4317-9b6b-818489033b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set devices and epochs\n",
    "args.accelerator = \"gpu\"\n",
    "args.devices = 1\n",
    "args.max_epochs = 30\n",
    "args.num_sanity_val_steps=0 \n",
    "# args.ckpt_path = \"/home/jupyter-group3/reconstruction/reconstruction-deep-network/reconstruction_deep_network/notebooks/pipelines/lightning_logs/version_17/checkpoints/epoch=epoch=0-loss=train_loss=0.1843.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99661ae1-1ef1-441a-bd75-fca91a263597",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e0919-7f02-41a3-9b7e-a6274fea81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kernel",
   "language": "python",
   "name": "venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
