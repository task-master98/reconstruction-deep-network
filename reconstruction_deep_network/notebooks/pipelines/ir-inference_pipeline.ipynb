{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1bbbea-cbd7-4493-9870-f3f6e4567aec",
   "metadata": {},
   "source": [
    "## **Inference Pipeline**\n",
    "This notebook will be used to test the model performance and test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad742d69-3a03-44e2-b659-06526832fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jupyter-group3/reconstruction/reconstruction-deep-network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3949ea8-6379-45ca-bfd1-693f17a2214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from argparse import ArgumentParser\n",
    "import yaml\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import reconstruction_deep_network\n",
    "from reconstruction_deep_network.data_loader.custom_loader import CustomDataLoader\n",
    "from reconstruction_deep_network.trainer.trainer import ModelTrainer\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976235cd-a8d7-41b7-856b-e8f74c42e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_checkpoint_files(directory, match_string):\n",
    "    checkpoint_files = []\n",
    "    pattern = re.compile(match_string)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        if pattern.search(filename):\n",
    "            checkpoint_files.append(os.path.join(directory, filename))\n",
    "\n",
    "    return checkpoint_files\n",
    "\n",
    "def show_images(images, n_views=4):\n",
    "    n_images = images.shape[0]\n",
    "    assert n_images == n_views\n",
    "    original_images = (images + 1) * 127.5\n",
    "    original_images = np.clip(original_images, 0, 255).astype(np.uint8)\n",
    "    n_rows = 2\n",
    "    n_cols = n_views//2 \n",
    "    if n_views > 2:\n",
    "        prods = list(itertools.product(range(n_rows), range(n_cols)))\n",
    "    else:\n",
    "        prods = [0, 1]\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(11, 5))\n",
    "    for itr, ij in enumerate(prods):\n",
    "        axs[ij].imshow((original_images[itr]))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_batch(data_dict, device = \"cpu\"):\n",
    "    batched_keys = [\"images\", \"text_embedding\", \"img_encoding\", \"R\", \"K\"]\n",
    "    num_views = data_dict[\"images\"].shape[0] \n",
    "    \n",
    "    \n",
    "    for key in data_dict:\n",
    "        if key in batched_keys:\n",
    "            data_dict[key] = torch.from_numpy(np.expand_dims(data_dict[key], 0)).to(device)    \n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "@torch.no_grad()\n",
    "def class_free_guidance_pair(latents, timestep, prompt_embed, R, K):\n",
    "    latents = torch.cat([latents] * 2)\n",
    "    timestep = torch.cat([timestep] * 2)\n",
    "\n",
    "    R = torch.cat([R] * 2)\n",
    "    K = torch.cat([K] * 2)\n",
    "\n",
    "    meta = {\n",
    "        \"K\": K,\n",
    "        \"R\": R\n",
    "    }\n",
    "\n",
    "    return latents, timestep, prompt_embed, meta\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_class_free(latents_high_res, timestep, prompt_embed, R, K, model):\n",
    "    \n",
    "    latents, timestep, prompt_embed, meta = class_free_guidance_pair(\n",
    "        latents_high_res, timestep, prompt_embed, R, K\n",
    "    )\n",
    "    print(timestep.device)\n",
    "    noise_pred = model(latents, timestep, prompt_embed, meta)\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + 9 * \\\n",
    "        (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    return noise_pred\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference_latent_prediction(model_trainer, prompt_embedding, R, K, device):\n",
    "    m = 8\n",
    "    height = 512\n",
    "    width = 512\n",
    "    to_torch = lambda x: torch.from_numpy(np.expand_dims(x, 0)).to(device)\n",
    "    prompt_embedding = to_torch(prompt_embedding)\n",
    "    R = to_torch(R)\n",
    "    K = to_torch(K)\n",
    "    latents = torch.randn(\n",
    "            1, m, 4, height//8, width//8, device=device)\n",
    "    \n",
    "    null_embedding = model_trainer.load_null_embedding()\n",
    "    null_embedding = null_embedding[:, None].repeat(1, m, 1, 1).to(device)        \n",
    "    complete_embedding = torch.cat([null_embedding, prompt_embedding])\n",
    "    \n",
    "    \n",
    "    model_trainer.scheduler.set_timesteps(50, device = device)\n",
    "    timesteps = model_trainer.scheduler.timesteps\n",
    "    \n",
    "    for i, t in enumerate(tqdm(timesteps)):        \n",
    "        _timestep = torch.cat([t[None, None]]*m, dim=1)\n",
    "\n",
    "        noise_pred = forward_class_free(\n",
    "            latents, _timestep, complete_embedding, R, K, model_trainer.mv_base_model)\n",
    "\n",
    "        latents = self.scheduler.step(\n",
    "            noise_pred, t, latents).prev_sample\n",
    "    \n",
    "    return latents\n",
    "    \n",
    "def create_ordered_dict(state_dict):\n",
    "    \n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k.replace(\"mv_base_model.unet.\", \"\") # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    return new_state_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632a5b1-0e46-456a-850b-ee96def7d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe1861-3118-46c1-a104-ae21b63e0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654eb47e-48e4-4615-995e-6de4797f6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_dir = reconstruction_deep_network.__path__[0]\n",
    "notebooks_dir = os.path.join(module_dir, \"notebooks\", \"pipelines\")\n",
    "lightning_logs = os.path.join(notebooks_dir, \"lightning_logs\")\n",
    "model_version = input(\"Enter model version\")\n",
    "model_dir = os.path.join(lightning_logs, f\"version_{model_version}\", \"checkpoints\")\n",
    "\n",
    "checkpoint_files = find_checkpoint_files(model_dir, r\"epoch=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b16a0-dd70-41fc-83bb-74ff239aa191",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357210f-ddd0-4224-8b05-a6ddeced4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cpu')\n",
    "# unet_model = torch.load(checkpoint_files[0], map_location='cpu')[\"state_dict\"]\n",
    "# unet_model = create_ordered_dict(unet_model)\n",
    "model_trainer = ModelTrainer()\n",
    "model_trainer.load_state_dict(torch.load(checkpoint_files[0], map_location='cuda')[\n",
    "            'state_dict'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b59677-dd98-4184-8df3-a89c3a21a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load VAE\n",
    "# model_version = input(\"Enter model version\")\n",
    "# model_dir = os.path.join(lightning_logs, f\"version_{model_version}\", \"checkpoints\")\n",
    "\n",
    "# checkpoint_files = find_checkpoint_files(model_dir, r\"epoch=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa22e83-e784-4645-8347-49911f23ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b2feb5-7124-4660-a336-e04bfad836cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model = torch.load(checkpoint_files[0], map_location='cpu')[\"state_dict\"]\n",
    "# vae_state_dict = OrderedDict()\n",
    "\n",
    "# for k, v in trained_model.items():\n",
    "#     if k.startswith(\"vae\"):\n",
    "#         name = k.replace(\"vae.\", \"\")\n",
    "#         vae_state_dict[name] = v\n",
    "\n",
    "# model_trainer.vae.load_state_dict(vae_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640f0c4-ca70-42f5-afdb-7ba37d53960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.diffusion_timestep = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d418b-1193-4ecc-b260-c3c7e9c63019",
   "metadata": {},
   "outputs": [],
   "source": [
    "for down_block in model_trainer.mv_base_model.unet.down_blocks:\n",
    "    if hasattr(down_block, 'has_cross_attention') and down_block.has_cross_attention:\n",
    "        down_block.has_cross_attention = False\n",
    "    \n",
    "    \n",
    "for up_block in model_trainer.mv_base_model.unet.up_blocks:\n",
    "    if hasattr(up_block, 'has_cross_attention') and up_block.has_cross_attention:\n",
    "        up_block.has_cross_attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3fd5a-5b76-48dc-8e26-38b21ba8ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name, param in model_trainer.mv_base_model.unet.named_parameters():\n",
    "    print(f'{param_name}: {param.requires_grad}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18066c6-ad69-44c4-96ba-004adc0df306",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load validation dataset\n",
    "val_metadata = \"ir-20231129-train-split\"\n",
    "num_views = 1\n",
    "val_dataset = CustomDataLoader(mode = \"train\",\n",
    "                               debug = False,\n",
    "                               metadata_filename = val_metadata,\n",
    "                               num_views = num_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db64ba4-80e4-4f00-9492-d93ba4331bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_index = np.random.randint(0, len(val_dataset))\n",
    "data_dict = val_dataset[1000]\n",
    "images = data_dict[\"images\"]\n",
    "\n",
    "# show_images(images, num_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2fb95-71d8-499e-89fb-c1bf109a379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join(module_dir, \"notebooks\", \"pipelines\", \"results\")\n",
    "if os.path.isdir(results_dir):\n",
    "    print(\"Result directory exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5212f80-b9c6-40ff-a890-7206bc558ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.mv_base_model.unet.to(\"cuda\")\n",
    "model_trainer.vae.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ec117-bba7-4991-81d3-6afb19ae4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predicted_latents = inference_latent_prediction(model_trainer, data_dict[\"text_embedding\"], data_dict[\"R\"], data_dict[\"K\"], device)\n",
    "# for itr, data_dict in enumerate(val_dataset):\n",
    "    \n",
    "    \n",
    "#     pseudo_batch = create_batch(data_dict, \"cuda\")\n",
    "#     images_pred = model_trainer.inference(pseudo_batch)\n",
    "    \n",
    "# #     predicted_latents = predicted_latents.to(\"cuda:0\")\n",
    "# #     print(predicted_latents.shape)\n",
    "# #     images_pred = model_trainer.decode_latent(predicted_latents, model_trainer.vae)\n",
    "#     images_pred = images_pred.cpu().numpy().squeeze()\n",
    "    \n",
    "# #     img_path = data_dict[\"images_paths\"][0].split(\"/\")\n",
    "# #     scan_id = img_path[0]\n",
    "# #     img_name = img_path[-1].split(\"_\")[0]\n",
    "    \n",
    "# #     file_name = os.path.join(results_dir, f\"{scan_id}_{img_name}.npz\")\n",
    "# #     np.savez_compressed(file_name, predictions=images_pred)\n",
    "    \n",
    "# #     del predicted_latents, images_pred\n",
    "#     if (itr + 2) % 1 == 0:\n",
    "#         print(\"Prediction generation done\")\n",
    "#         break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442e6fb-095d-4a3f-a32d-f19d7e8324e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1350 3000 1310 \n",
    "val_list = [1300, 1310, 1350, 3000, 1399, 1400, 1599]\n",
    "pred_list = []\n",
    "for val_idx in tqdm(val_list):\n",
    "    data_dict = val_dataset[val_idx]\n",
    "    pseudo_batch = create_batch(data_dict, \"cuda\")\n",
    "    images_pred = model_trainer.inference(pseudo_batch)\n",
    "    break\n",
    "    pred_list.append(images_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c32e9d-3c8e-4f9c-b482-54163510fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d897c-3e33-46d6-bc66-e85237a7c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_plot = [pred_list[1], pred_list[2], pred_list[3]]\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize = (12, 5))\n",
    "\n",
    "grid_list = [[0, 0], [0, 1], [0, 2]]\n",
    "for itr, ij in enumerate(grid_list):\n",
    "    image = images_to_plot[itr].cpu().numpy().squeeze()\n",
    "    axs[itr].imshow(image)\n",
    "    break\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d4abb-408f-4ea8-a786-021273c086b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d18c4-3c59-409a-a29e-57ebfdd3a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "plt.imshow(images_pred.cpu().numpy().squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6f66e-1ac2-413c-8667-e305f217683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(data_dict[\"images\"].cpu().numpy().squeeze(), 1)\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.imshow(data_dict[\"images\"].cpu().numpy().squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f4687-8c40-4783-b6d7-0fbcd1a558b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b6ad1-8e2f-4227-91b8-3d8fefb67f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "categories = ['50 Timesteps', '100 Timesteps', '500 Timesteps', '1000 Timesteps']\n",
    "values = [3.3, 6, 27, 63]\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(categories, values, color='orange')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Diffusion Timesteps')\n",
    "plt.ylabel('Runtime')\n",
    "plt.title('Runtime Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e670f-694e-422a-8392-11734f447a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff6d3f-4f05-4d49-8b30-8daa6edf168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load npz file\n",
    "file_list = os.listdir(results_dir)\n",
    "file_name = file_list[7]\n",
    "file_path = os.path.join(results_dir, file_name)\n",
    "image_arr = np.load(file_path)[\"predictions\"]\n",
    "image_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ac0c0-933c-4b52-bb2d-cc4bb0dd6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr = image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc72cf-ee94-4d86-88d4-e9b3835dfbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(image_arr, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f7d20-f922-49d0-8e6a-8443936015fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72be5a-a315-40fc-9d5e-d8c7be4e4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[6][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e52d0-d780-4877-b573-78fa7308f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d602b-ba5d-4f36-987e-6b04a9131fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 1124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022435f2-0a16-4de8-b4ce-a6a5336d7c31",
   "metadata": {},
   "source": [
    "## **TODO: Error analysis with decoder**\n",
    "Check whether the autoencoder can actually decode the latents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a09db7-2d2d-4237-8f29-e9624f612fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "m = nn.Linear(192, 10)\n",
    "input = torch.randn(2, 197, 192)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd35b06-78a6-4604-b425-85e24393427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten(1)\n",
    "x = torch.randn(1, 1024, 8, 8)\n",
    "op = flatten(x)\n",
    "op.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147a9bf-8fbb-4387-a256-d2803b0b92fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kernel",
   "language": "python",
   "name": "venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
