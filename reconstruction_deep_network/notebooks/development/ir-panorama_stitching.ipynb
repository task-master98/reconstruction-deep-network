{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Panorama Stitching**\n",
    "This notebook will be responsible for creating the dataset required for our generative model. The input will be the panoramic image and the output will be a sequence of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from matplotlib.image import imread\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import reconstruction_deep_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_dir = reconstruction_deep_network.__path__[0]\n",
    "data_dir = os.path.join(os.path.dirname(module_dir), \"data\", \"v1\", \"tasks\", \"region_classification\", \"data\")\n",
    "panorama_dir = os.path.join(data_dir, \"mp_sb\")\n",
    "metadata_dir = os.path.join(module_dir, \"metadata\")\n",
    "feature_metadata_df = pd.read_csv(os.path.join(metadata_dir, \"feature_metadata.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panorama_ids = feature_metadata_df[\"panorama_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def split_panorama(panorama, size):\n",
    "    \"\"\"\n",
    "    Splits a panorama image into a sequence of smaller images (vectorized).\n",
    "\n",
    "    Args:\n",
    "    - panorama: A NumPy array representing the panorama image.\n",
    "    - size: A tuple (width, height) specifying the size of the smaller images.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the sequence of smaller images as NumPy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    panorama_height, panorama_width, _ = panorama.shape\n",
    "    width, height = size\n",
    "\n",
    "    if panorama_width % width != 0 or panorama_height % height != 0:\n",
    "        raise ValueError(\"Image size does not evenly divide the panorama dimensions.\")\n",
    "\n",
    "    num_rows = panorama_height // height\n",
    "    num_cols = panorama_width // width\n",
    "\n",
    "    # Calculate row and column indices for slicing\n",
    "    rows = np.arange(num_rows)\n",
    "    cols = np.arange(num_cols)\n",
    "\n",
    "    # Use meshgrid to generate all possible indices\n",
    "    col_indices, row_indices = np.meshgrid(cols, rows)\n",
    "\n",
    "    # Calculate slicing coordinates\n",
    "    x_start = col_indices * width\n",
    "    x_end = (col_indices + 1) * width\n",
    "    y_start = row_indices * height\n",
    "    y_end = (row_indices + 1) * height\n",
    "\n",
    "    # Slice the panorama image to extract smaller images\n",
    "    small_images = [panorama[y:y_end, x:x_end] for x, y, x_end, y_end in zip(x_start.ravel(), y_start.ravel(), x_end.ravel(), y_end.ravel())]\n",
    "\n",
    "    return tuple(small_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id = \"17DRP5sb8fy\"\n",
    "test_pano = panorama_ids[0]\n",
    "test_img = load_image(scan_id, test_pano)\n",
    "print(test_img.shape)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.imshow(test_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (128, 128)\n",
    "sequence_tuple = split_panorama(test_img, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define NERF Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, camera_intrinsics, camera_extrinsics, hn, hf, images, chunk_size=10, img_index=0, nb_bins=192, H=400,\n",
    "         W=400):\n",
    "    ray_origins, ray_directions, _ = sample_batch(camera_extrinsics, camera_intrinsics, images, None, H, W,\n",
    "                                                  img_index=img_index, sample_all=True)\n",
    "    data = []\n",
    "    for i in range(int(np.ceil(H / chunk_size))):\n",
    "        ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(camera_intrinsics.device)\n",
    "        ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(camera_intrinsics.device)\n",
    "        regenerated_px_values = render_rays(model, ray_origins_, ray_directions_, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "        data.append(regenerated_px_values)\n",
    "    img = torch.cat(data).data.cpu().numpy().reshape(H, W, 3)\n",
    "    # plt.imshow(img)\n",
    "    # plt.show()\n",
    "    return img\n",
    "    # plt.savefig(f'Imgs/novel_view.png', bbox_inches='tight')\n",
    "    # plt.close()\n",
    "\n",
    "class NerfModel(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos=10, embedding_dim_direction=4, hidden_dim=128):\n",
    "        super(NerfModel, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), )\n",
    "        self.block2 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + hidden_dim + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim + 1), )\n",
    "        self.block3 = nn.Sequential(nn.Linear(embedding_dim_direction * 6 + hidden_dim + 3, hidden_dim // 2),\n",
    "                                    nn.ReLU(), )\n",
    "        self.block4 = nn.Sequential(nn.Linear(hidden_dim // 2, 3), nn.Sigmoid(), )\n",
    "\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.embedding_dim_direction = embedding_dim_direction\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_direction)\n",
    "        tmp = self.block2(torch.cat((self.block1(emb_x), emb_x), dim=1))\n",
    "        h, sigma = tmp[:, :-1], self.relu(tmp[:, -1])\n",
    "        c = self.block4(self.block3(torch.cat((h, emb_d), dim=1)))\n",
    "        return c, sigma\n",
    "\n",
    "\n",
    "def compute_accumulated_transmittance(alphas):\n",
    "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
    "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
    "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
    "\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
    "    device = ray_origins.device\n",
    "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device=device)  # Perturb sampling along each ray.\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
    "\n",
    "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)  # [batch_size, nb_bins, 3]\n",
    "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
    "\n",
    "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
    "    alpha = 1 - torch.exp(-sigma.reshape(x.shape[:-1]) * delta)  # [batch_size, nb_bins]\n",
    "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
    "    return (weights * colors.reshape(x.shape)).sum(dim=1)  # Pixel values\n",
    "\n",
    "\n",
    "def train(nerf_model, optimizers, schedulers, training_images, camera_extrinsics, camera_intrinsics, batch_size,\n",
    "          nb_epochs, hn=0., hf=1., nb_bins=192):\n",
    "    H, W = training_images.shape[1:3]\n",
    "\n",
    "    training_loss = []\n",
    "    for _ in tqdm(range(nb_epochs)):\n",
    "        ids = np.arange(training_images.shape[0])\n",
    "        np.random.shuffle(ids)\n",
    "        for img_index in ids:\n",
    "            rays_o, rays_d, samples_idx = sample_batch(camera_extrinsics, camera_intrinsics, training_images,\n",
    "                                                       batch_size, H, W, img_index=img_index)\n",
    "            gt_px_values = torch.from_numpy(training_images[samples_idx]).to(camera_intrinsics.device)\n",
    "            regenerated_px_values = render_rays(nerf_model, rays_o, rays_d, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "            loss = ((gt_px_values - regenerated_px_values) ** 2).sum()\n",
    "\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.step()\n",
    "            training_loss.append(loss.item())\n",
    "        for scheduler in schedulers:\n",
    "            scheduler.step()\n",
    "    return training_loss\n",
    "\n",
    "\n",
    "def initialize_camera_parameters(images, device='cpu'):\n",
    "    camera_intrinsics = torch.ones(1, device=device, requires_grad=True)\n",
    "    camera_extrinsics = torch.zeros((images.shape[0], 6), device=device, dtype=torch.float32, requires_grad=True)\n",
    "    return camera_intrinsics, camera_extrinsics\n",
    "\n",
    "def load_image(scan_id: str, panorama_id: str):\n",
    "    file_path = os.path.join(panorama_dir, scan_id, f\"{panorama_id}.jpg\")\n",
    "    file_img = Image.open(file_path)\n",
    "    return np.array(file_img)\n",
    "\n",
    "def load_images(scan_id: str, panorama_id: str):\n",
    "    pano_img = load_image(scan_id, panorama_id)\n",
    "    img_tuple = split_panorama(pano_img, (128, 128))\n",
    "    images = None\n",
    "    for i, split_img in enumerate(img_tuple):\n",
    "        img = np.expand_dims(split_img, 0)\n",
    "        images = np.concatenate((images, img)) if images is not None else img\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_ndc_rays(H, W, focal, rays_o, rays_d, near=1.):\n",
    "    # We shift o to the ray’s intersection with the near plane at z = −n (before the NDC conversion)\n",
    "    t = -(near + rays_o[..., 2]) / rays_d[..., 2]\n",
    "    rays_o = rays_o + t[..., None] * rays_d\n",
    "\n",
    "    rays_o = torch.stack([- focal / W / 2. * rays_o[..., 0] / rays_o[..., 2],\n",
    "                          - focal / H / 2. * rays_o[..., 1] / rays_o[..., 2],\n",
    "                          1. + 2. * near / rays_o[..., 2]], -1)  # Eq 25 https://arxiv.org/pdf/2003.08934.pdf\n",
    "    rays_d = torch.stack([- focal / W / 2. * (rays_d[..., 0] / rays_d[..., 2] - rays_o[..., 0] / rays_o[..., 2]),\n",
    "                          - focal / H / 2. * (rays_d[..., 1] / rays_d[..., 2] - rays_o[..., 1] / rays_o[..., 2]),\n",
    "                          - 2. * near / rays_o[..., 2]], -1)  # Eq 26 https://arxiv.org/pdf/2003.08934.pdf\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def sample_batch(camera_extrinsics, camera_intrinsics, images, batch_size, H, W, img_index=0, sample_all=False):\n",
    "    if sample_all:\n",
    "        image_indices = (torch.zeros(W * H) + img_index).type(torch.long)\n",
    "        u, v = np.meshgrid(np.linspace(0, W - 1, W, dtype=int), np.linspace(0, H - 1, H, dtype=int))\n",
    "        u = torch.from_numpy(u.reshape(-1)).to(camera_intrinsics.device)\n",
    "        v = torch.from_numpy(v.reshape(-1)).to(camera_intrinsics.device)\n",
    "    else:\n",
    "        image_indices = (torch.zeros(batch_size) + img_index).type(torch.long)  # Sample random images\n",
    "        u = torch.randint(W, (batch_size,), device=camera_intrinsics.device)  # Sample random pixels\n",
    "        v = torch.randint(H, (batch_size,), device=camera_intrinsics.device)\n",
    "\n",
    "    focal = camera_intrinsics[0] ** 2 * W\n",
    "    t = camera_extrinsics[img_index, :3]\n",
    "    r = camera_extrinsics[img_index, -3:]\n",
    "\n",
    "    # Creating the c2w matrix, Section 4.1 from the paper\n",
    "    phi_skew = torch.stack([torch.cat([torch.zeros(1, device=r.device), -r[2:3], r[1:2]]),\n",
    "                            torch.cat([r[2:3], torch.zeros(1, device=r.device), -r[0:1]]),\n",
    "                            torch.cat([-r[1:2], r[0:1], torch.zeros(1, device=r.device)])], dim=0)\n",
    "    alpha = r.norm() + 1e-15\n",
    "    R = torch.eye(3, device=r.device) + (torch.sin(alpha) / alpha) * phi_skew + (\n",
    "            (1 - torch.cos(alpha)) / alpha ** 2) * (phi_skew @ phi_skew)\n",
    "    c2w = torch.cat([R, t.unsqueeze(1)], dim=1)\n",
    "    c2w = torch.cat([c2w, torch.tensor([[0., 0., 0., 1.]], device=c2w.device)], dim=0)\n",
    "\n",
    "    rays_d_cam = torch.cat([((u.to(camera_intrinsics.device) - .5 * W) / focal).unsqueeze(-1),\n",
    "                            (-(v.to(camera_intrinsics.device) - .5 * H) / focal).unsqueeze(-1),\n",
    "                            - torch.ones_like(u).unsqueeze(-1)], dim=-1)\n",
    "    rays_d_world = torch.matmul(c2w[:3, :3].view(1, 3, 3), rays_d_cam.unsqueeze(2)).squeeze(2)\n",
    "    rays_o_world = c2w[:3, 3].view(1, 3).expand_as(rays_d_world)\n",
    "    rays_o_world, rays_d_world = get_ndc_rays(H, W, focal, rays_o=rays_o_world, rays_d=rays_d_world)\n",
    "    return rays_o_world, F.normalize(rays_d_world, p=2, dim=1), (image_indices, v.cpu(), u.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "nb_epochs = 1000\n",
    "\n",
    "training_images = load_images(scan_id, test_pano)\n",
    "camera_intrinsics, camera_extrinsics = initialize_camera_parameters(training_images, device=device)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NerfModel(hidden_dim=256).to(device)\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer_camera_parameters = torch.optim.Adam({camera_extrinsics}, lr=0.0009)\n",
    "optimizer_focal = torch.optim.Adam({camera_intrinsics}, lr=0.001)\n",
    "scheduler_model = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    model_optimizer, [10 * (i + 1) for i in range(nb_epochs // 10)], gamma=0.9954)\n",
    "scheduler_camera_parameters = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer_camera_parameters, [100 * (i + 1) for i in range(nb_epochs // 100)], gamma=0.81)\n",
    "scheduler_focal = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer_focal, [100 * (i + 1) for i in range(nb_epochs // 100)], gamma=0.9)\n",
    "train(model, [model_optimizer, optimizer_camera_parameters, optimizer_focal],\n",
    "        [scheduler_model, scheduler_camera_parameters, scheduler_focal], training_images, camera_extrinsics,\n",
    "        camera_intrinsics, batch_size, nb_epochs, hn=0., hf=1., nb_bins=192)\n",
    "\n",
    "# Part 2\n",
    "model = NerfModel(hidden_dim=256).to(device)\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler_model = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    model_optimizer, [10 * (i + 1) for i in range(nb_epochs // 10)], gamma=0.9954)\n",
    "train(model, [model_optimizer], [scheduler_model], training_images, camera_extrinsics, camera_intrinsics,\n",
    "        batch_size, nb_epochs, hn=0., hf=1., nb_bins=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test(model, camera_intrinsics, (.5 * camera_extrinsics[0] + .5 * camera_extrinsics[1]).unsqueeze(0), 0., 1.,\n",
    "         training_images, img_index=0, nb_bins=192, H=training_images.shape[1], W=training_images.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_image = (img * 255).astype(np.uint8)\n",
    "\n",
    "# Convert the scaled image to BGR color format\n",
    "plt.imshow(scaled_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reconstruction-deep-network-Kqq14QZk-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
